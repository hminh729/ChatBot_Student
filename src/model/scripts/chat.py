import ollama
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# ğŸ› ï¸ Load database tá»« Chroma
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
vector_db = Chroma(persist_directory="db", embedding_function=embedding_model)

# ğŸ”¥ HÃ m há»i Ä‘Ã¡p vá»›i chatbot
def ask_bot(query):
    # ğŸ” TÃ¬m kiáº¿m ná»™i dung liÃªn quan trong ChromaDB
    docs = vector_db.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # ğŸ¯ Táº¡o prompt vá»›i dá»¯ liá»‡u tá»« giÃ¡o trÃ¬nh
    prompt = f"ThÃ´ng tin tá»« giÃ¡o trÃ¬nh:\n{context}\n\nCÃ¢u há»i: {query}\nTráº£ lá»i chi tiáº¿t:"
    
    # ğŸ§  Gá»­i vÃ o DeepSeek (hoáº·c báº¥t ká»³ model nÃ o trong Ollama)
    response = ollama.chat(model="deepseek", messages=[{"role": "user", "content": prompt}])
    
    return response["message"]["content"]

# ğŸ’¬ Cháº¡y chatbot trong terminal
while True:
    user_input = input("Báº¡n: ")
    if user_input.lower() in ["exit", "quit"]:
        break
    print("AI:", ask_bot(user_input))
