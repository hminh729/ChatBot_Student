import ollama
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# Káº¿t ná»‘i vá»›i ChromaDB
def load_vector_db(db_path="db"):
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
    return Chroma(persist_directory=db_path, embedding_function=embedding_model)

def retrieve_context(vector_db, query):
    """Truy váº¥n dá»¯ liá»‡u tá»« ChromaDB Ä‘á»ƒ láº¥y thÃ´ng tin liÃªn quan"""
    results = vector_db.similarity_search(query, k=3)  # Láº¥y 3 káº¿t quáº£ gáº§n nháº¥t
    context = "\n".join([doc.page_content for doc in results])
    return context if context else "KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u liÃªn quan."

def chat_with_ollama(model="deepseek-r1:1.5b ", db_path="db"):
    print("ğŸ¤– AI Chatbot vá»›i ChromaDB (Nháº­p 'exit' Ä‘á»ƒ dá»«ng)\n")

    vector_db = load_vector_db(db_path)  # Load database

    while True:
        user_input = input("Báº¡n: ")

        if user_input.lower() == "exit":
            print("ğŸ‘‹ Táº¡m biá»‡t!")
            break  # Dá»«ng chat

        # Truy xuáº¥t dá»¯ liá»‡u tá»« ChromaDB
        context = retrieve_context(vector_db, user_input)

        # Gá»­i tin nháº¯n Ä‘áº¿n Ollama vá»›i dá»¯ liá»‡u tá»« ChromaDB
        response = ollama.chat(
            model=model,
            messages=[
                {"role": "system", "content": "Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh, hÃ£y tráº£ lá»i chÃ­nh xÃ¡c."},
                {"role": "user", "content": f"CÃ¢u há»i: {user_input}\nDá»¯ liá»‡u liÃªn quan:\n{context}"}
            ]
        )

        print(f"AI: {response['message']['content']}\n")

if __name__ == "__main__":
    chat_with_ollama(model="gemma3:1b", db_path="db")
